{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Implement of Optimus pipeline\n",
    "#### Pipeline version v6.0.0 with output in h5ad format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. The wld files used in this pipeline are downloaded from the following link:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### https://github.com/broadinstitute/warp/blob/develop/pipelines/skylab/optimus/Optimus.wdl\n",
    "###### https://github.com/broadinstitute/warp/blob/develop/tasks/skylab/H5adUtils.wdl\n",
    "###### https://github.com/broadinstitute/warp/blob/develop/tasks/skylab/FastqProcessing.wdl\n",
    "###### https://github.com/broadinstitute/warp/blob/develop/tasks/skylab/StarAlign.wdl\n",
    "###### https://github.com/broadinstitute/warp/blob/develop/tasks/skylab/Metrics.wdl\n",
    "###### https://github.com/broadinstitute/warp/blob/develop/tasks/skylab/RunEmptyDrops.wdl\n",
    "###### https://github.com/broadinstitute/warp/blob/develop/tasks/skylab/CheckInputs.wdl\n",
    "###### https://github.com/broadinstitute/warp/blob/develop/tasks/skylab/MergeSortBam.wdl\n",
    "###### https://github.com/broadinstitute/CellBender/blob/master/wdl/cellbender_remove_background.wdl\n",
    "\n",
    "#### recent update information\n",
    "###### Optimus.wdl: Aug 31, 2023 pipeline_version = \"6.0.0\"\n",
    "###### H5adUtils.wdl: Aug 28, 2023\n",
    "###### StarAlign.wdl: Aug 28, 2023\n",
    "###### Metrics.wdl: Aug 31, 2023\n",
    "###### CellBender v0.3.0: Aug 8, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A comprehensive gene annotation (PRI) is used in this pipeline: https://www.gencodegenes.org/human/release_27.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Check the difference between the wdl files in pipeline_version 5.7.4 and 6.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The pipeline version 5.7.4 is in branch sc-pip and the 6.0.0 is in branch sc-pip-v2\n",
    "###### The following wld files are named with version information for the comparison, the pipeline, the version information (_v6.0.0 will be removed in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/xiliu/Documents/analysis/terraPipelines/notebook\n"
     ]
    }
   ],
   "source": [
    "### current directory ###\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6a7\n",
      "> import \"../tasks/LoomUtils.wdl\" as LoomUtils\n",
      "9d9\n",
      "< import \"../tasks/H5adUtils.wdl\" as H5adUtils\n",
      "11a12\n",
      "> \n",
      "13c14\n",
      "<     description: \"The optimus 3' pipeline processes 10x genomics sequencing data based on the v2 chemistry. It corrects cell barcodes and UMIs, aligns reads, marks duplicates, and returns data as alignments in BAM format and as counts in sparse matrix exchange format.\"\n",
      "---\n",
      ">     description: \"The optimus 3' pipeline processes 10x genomics sequencing data based on v2 or v3 chemistry. It corrects cell barcodes and UMIs, aligns reads, marks duplicates, and returns data as alignments in BAM format and as counts in sparse matrix exchange format.\"\n",
      "40,42d40\n",
      "<     # read_structure is based on v2 or v3 chemistry\n",
      "<     String read_struct = checkOptimusInput.read_struct_out\n",
      "< \n",
      "53,54c51,52\n",
      "<     # Set to Forward, Reverse, or Unstranded to account for stranded library preparations (per STARsolo documentation)\n",
      "<     String star_strand_mode = \"Forward\"\n",
      "---\n",
      ">     # Set to true to count reads in stranded mode\n",
      ">     String use_strand_info = \"false\"\n",
      "65a64\n",
      ">   String pipeline_version = \"5.7.4\"\n",
      "67d65\n",
      "<   String pipeline_version = \"6.0.0\"\n",
      "73,74c71,72\n",
      "<   File whitelist_v2 = \"gs://gcp-public-data--broad-references/RNA/resources/737k-august-2016.txt\"\n",
      "<   File whitelist_v3 = \"gs://gcp-public-data--broad-references/RNA/resources/3M-febrary-2018.txt\"\n",
      "---\n",
      ">   File whitelist_v2 = \"gs://whitelabgx-references/resources/X10_resources/737k-august-2016.txt\"\n",
      ">   File whitelist_v3 = \"gs://whitelabgx-references/resources/X10_resources/3M-febrary-2018.txt\"\n",
      "92c90\n",
      "<     star_strand_mode: \"STAR mode for handling stranded reads. Options are 'Forward', 'Reverse, or 'Unstranded.' Default is Forward.\"\n",
      "---\n",
      ">     use_strand_info: \"Set to true to count reads in stranded mode\"\n",
      "119,120c117\n",
      "<       sample_id = input_id,\n",
      "<       read_struct = read_struct\n",
      "---\n",
      ">       sample_id = input_id\n",
      "128d124\n",
      "<         star_strand_mode = star_strand_mode,\n",
      "163,166d158\n",
      "<       cell_reads = STARsoloFastq.cell_reads,\n",
      "<       summary = STARsoloFastq.summary,\n",
      "<       align_features = STARsoloFastq.align_features,\n",
      "<       umipercell = STARsoloFastq.umipercell,\n",
      "180c172\n",
      "<     call H5adUtils.OptimusH5adGeneration{\n",
      "---\n",
      ">     call LoomUtils.OptimusLoomGeneration{\n",
      "203d194\n",
      "<         cell_reads = STARsoloFastq.cell_reads_sn_rna,\n",
      "206c197\n",
      "<     call H5adUtils.SingleNucleusOptimusH5adOutput as OptimusH5adGenerationWithExons{\n",
      "---\n",
      ">     call LoomUtils.SingleNucleusOptimusLoomOutput as OptimusLoomGenerationWithExons{\n",
      "222a214\n",
      "> \n",
      "225c217\n",
      "<   File final_h5ad_output = select_first([OptimusH5adGenerationWithExons.h5ad_output, OptimusH5adGeneration.h5ad_output])\n",
      "---\n",
      ">   File final_loom_output = select_first([OptimusLoomGenerationWithExons.loom_output, OptimusLoomGeneration.loom_output])\n",
      "227d218\n",
      "< \n",
      "239,242c230,231\n",
      "<     File? aligner_metrics = MergeStarOutputs.cell_reads_out\n",
      "<     # h5ad\n",
      "<     File h5ad_output_file = final_h5ad_output\n",
      "<   }\n",
      "---\n",
      ">     # loom\n",
      ">     File loom_output_file = final_loom_output\n",
      "243a233\n",
      "> }\n",
      "\\ No newline at end of file\n"
     ]
    }
   ],
   "source": [
    "!diff ../scAtlas/optimus/Optimus_v6.0.0.wdl  ../scAtlas/optimus/Optimus.wdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111c111\n",
      "<     # Check for chemistry version to produce read structure and whitelist\n",
      "---\n",
      ">     \n",
      "116d115\n",
      "<       echo 16C10M > read_struct.txt\n",
      "121d119\n",
      "<       echo 16C12M > read_struct.txt\n",
      "153d150\n",
      "<     String read_struct_out = read_string(\"read_struct.txt\")\n"
     ]
    }
   ],
   "source": [
    "!diff ../scAtlas/tasks/checkInputs_v6.0.0.wdl ../scAtlas/tasks/checkInputs.wdl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11d10\n",
      "<     String read_struct\n",
      "14c13\n",
      "<     String docker = \"us.gcr.io/broad-gotc-prod/warp-tools:1.0.1-1686932671\"\n",
      "---\n",
      ">     String docker = \"us.gcr.io/broad-gotc-prod/warp-tools:1.0.1-1679490798\"\n",
      "33d31\n",
      "<     read_struct: \"read structure for the 10x chemistry. This automatically selected in the checkInputs task\"\n",
      "105a104,105\n",
      ">         --barcode-length 16 \\\n",
      ">         --umi-length $UMILENGTH \\\n",
      "109d108\n",
      "<         --read-structure \"~{read_struct}\" \\\n",
      "140c139\n",
      "<     String docker =  \"us.gcr.io/broad-gotc-prod/warp-tools:1.0.1-1686932671\"\n",
      "---\n",
      ">     String docker =  \"quay.io/humancellatlas/secondary-analysis-sctools:v0.3.14-test2\"\n",
      "234,261d232\n",
      "< \n",
      "< task FastqProcessATAC {\n",
      "< \n",
      "<     input {\n",
      "<         Array[String] read1_fastq\n",
      "<         Array[String] read3_fastq\n",
      "<         Array[String] barcodes_fastq\n",
      "<         String read_structure = \"16C\"\n",
      "<         String barcode_orientation = \"FIRST_BP_RC\"\n",
      "<         String output_base_name\n",
      "<         File whitelist\n",
      "< \n",
      "<         # [?] copied from corresponding optimus wdl for fastqprocessing\n",
      "<         # using the latest build of warp-tools in GCR\n",
      "<         String docker = \"us.gcr.io/broad-gotc-prod/warp-tools:aa_updatefastqprocess_cbtag\"\n",
      "< \n",
      "<         # Runtime attributes [?]\n",
      "<         Int mem_size = 5\n",
      "<         Int cpu = 16\n",
      "<         # TODO decided cpu\n",
      "<         # estimate that bam is approximately equal in size to fastq, add 20% buffer\n",
      "<         Int disk_size = ceil(2 * ( size(read1_fastq, \"GiB\") + size(read3_fastq, \"GiB\") + size(barcodes_fastq, \"GiB\") )) + 400\n",
      "<         Int preemptible = 3\n",
      "<     }\n",
      "< \n",
      "<     meta {\n",
      "<         description: \"Converts a set of fastq files to unaligned bam file/fastq file, also corrects barcodes and partitions the alignments by barcodes. Allows for variable barcode and umi lengths as input, if applicable.\"\n",
      "<     }\n",
      "263,358d233\n",
      "<     parameter_meta {\n",
      "<         read1_fastq: \"Array of read 1 FASTQ files of paired reads -- forward reads\"\n",
      "<         read3_fastq: \"Array of read 3 FASTQ files of paired reads -- reverse reads\"\n",
      "<         barcodes_fastq: \"Array of read 2 FASTQ files which contains the cellular barcodes\"\n",
      "<         output_base_name: \"Name of sample matching this file, inserted into read group header\"\n",
      "<         read_structure: \"A string that specifies the barcode (C) positions in the Read 2 fastq\"\n",
      "<         barcode_orientation: \"A string that specifies the orientation of barcode needed for scATAC data. The default is FIRST_BP. Other options include LAST_BP, FIRST_BP_RC or LAST_BP_RC.\"\n",
      "<         whitelist: \"10x genomics cell barcode whitelist for scATAC\"\n",
      "<         docker: \"(optional) the docker image containing the runtime environment for this task\"\n",
      "<         mem_size: \"(optional) the amount of memory (MiB) to provision for this task\"\n",
      "<         cpu: \"(optional) the number of cpus to provision for this task\"\n",
      "<         disk_size: \"(optional) the amount of disk space (GiB) to provision for this task\"\n",
      "<         preemptible: \"(optional) if non-zero, request a pre-emptible instance and allow for this number of preemptions before running the task on a non preemptible machine\"\n",
      "<     }\n",
      "< \n",
      "<     command <<<\n",
      "< \n",
      "<         set -e\n",
      "< \n",
      "<         declare -a FASTQ1_ARRAY=(~{sep=' ' read1_fastq})\n",
      "<         declare -a FASTQ2_ARRAY=(~{sep=' ' barcodes_fastq})\n",
      "<         declare -a FASTQ3_ARRAY=(~{sep=' ' read3_fastq})\n",
      "< \n",
      "<         read1_fastq_files=`printf '%s ' \"${FASTQ1_ARRAY[@]}\"; echo`\n",
      "<         read2_fastq_files=`printf '%s ' \"${FASTQ2_ARRAY[@]}\"; echo`\n",
      "<         read3_fastq_files=`printf '%s ' \"${FASTQ3_ARRAY[@]}\"; echo`\n",
      "< \n",
      "<         echo $read1_fastq_files\n",
      "<         \n",
      "<         mkdir /cromwell_root/input_fastq\n",
      "<         gcloud storage cp $read1_fastq_files /cromwell_root/input_fastq\n",
      "<         gcloud storage cp $read2_fastq_files /cromwell_root/input_fastq\n",
      "<         gcloud storage cp $read3_fastq_files /cromwell_root/input_fastq\n",
      "< \n",
      "<         # barcodes R2\n",
      "<         R1_FILES_CONCAT=\"\"\n",
      "<         for fastq in \"${FASTQ2_ARRAY[@]}\"\n",
      "<         do\n",
      "<             BASE=`basename $fastq`\n",
      "<             BASE=`echo --R1 /cromwell_root/input_fastq/$BASE`\n",
      "<             R1_FILES_CONCAT+=\"$BASE \"\n",
      "<         done\n",
      "<         echo $R1_FILES_CONCAT\n",
      "< \n",
      "<         # R1\n",
      "<         R2_FILES_CONCAT=\"\"\n",
      "<         for fastq in \"${FASTQ1_ARRAY[@]}\"\n",
      "<         do\n",
      "<             BASE=`basename $fastq`\n",
      "<             BASE=`echo --R2 /cromwell_root/input_fastq/$BASE`\n",
      "<             R2_FILES_CONCAT+=\"$BASE \"\n",
      "<         done\n",
      "<         echo $R2_FILES_CONCAT\n",
      "<         \n",
      "<         # R3\n",
      "<         R3_FILES_CONCAT=\"\"\n",
      "<         for fastq in \"${FASTQ3_ARRAY[@]}\"\n",
      "<         do\n",
      "<             BASE=`basename $fastq`\n",
      "<             BASE=`echo --R3 /cromwell_root/input_fastq/$BASE`\n",
      "<             R3_FILES_CONCAT+=\"$BASE \"\n",
      "<         done\n",
      "<         echo $R3_FILES_CONCAT\n",
      "< \n",
      "<         # Call fastq process\n",
      "<         # outputs fastq files where the corrected barcode is in the read name\n",
      "<         mkdir /cromwell_root/output_fastq\n",
      "<         cd /cromwell_root/output_fastq\n",
      "< \n",
      "<         fastqprocess \\\n",
      "<         --bam-size 30.0 \\\n",
      "<         --sample-id \"~{output_base_name}\" \\\n",
      "<         $R1_FILES_CONCAT \\\n",
      "<         $R2_FILES_CONCAT \\\n",
      "<         $R3_FILES_CONCAT \\\n",
      "<         --white-list \"~{whitelist}\" \\\n",
      "<         --output-format \"FASTQ\" \\\n",
      "<         --barcode-orientation \"~{barcode_orientation}\" \\\n",
      "<         --read-structure \"~{read_structure}\"\n",
      "< \n",
      "<     >>>\n",
      "< \n",
      "<     runtime {\n",
      "<         docker: docker\n",
      "<         cpu: cpu\n",
      "<         memory: \"${mem_size} MiB\"\n",
      "<         disks: \"local-disk ${disk_size} HDD\"\n",
      "<         preemptible: preemptible\n",
      "<     }\n",
      "< \n",
      "<     output {\n",
      "<         Array[File] fastq_R1_output_array = glob(\"/cromwell_root/output_fastq/fastq_R1_*\")\n",
      "<         Array[File] fastq_R3_output_array = glob(\"/cromwell_root/output_fastq/fastq_R3_*\")\n",
      "<     }\n",
      "< }\n",
      "< \n"
     ]
    }
   ],
   "source": [
    "!diff ../scAtlas/tasks/FastqProcessing_v6.0.0.wdl ../scAtlas/tasks/FastqProcessing.wdl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12c12\n",
      "<     String docker = \"us.gcr.io/broad-gotc-prod/warp-tools:1.0.6-1692962087\"\n",
      "---\n",
      ">     String docker = \"us.gcr.io/broad-gotc-prod/warp-tools:1.0.1-1679941323\"\n",
      "62c62,63\n",
      "<     mv updated.~{input_id}.cell-metrics.csv ~{input_id}.cell-metrics.csv\n",
      "---\n",
      ">     # remove the following columns: reads_unmapped, reads_mapped_exonic, reads_mapped_intronic, reads_mapped_utr, duplicate_reads, reads_mapped_intergenic\n",
      ">     cut -d',' -f 1-4,8-9,11-26,29-36 updated.~{input_id}.cell-metrics.csv > ~{input_id}.cell-metrics.csv\n",
      "88c89\n",
      "<     String docker = \"us.gcr.io/broad-gotc-prod/warp-tools:1.0.5-1692706846\"\n",
      "---\n",
      ">     String docker = \"us.gcr.io/broad-gotc-prod/warp-tools:1.0.1-1679941323\"\n",
      "128c129,130\n",
      "<     mv updated.~{input_id}.gene-metrics.csv ~{input_id}.gene-metrics.csv\n",
      "---\n",
      ">     # remove the following columns: reads_mapped_exonic, reads_mapped_intronic, reads_mapped_utr, duplicate_reads\n",
      ">     cut -d',' -f 1-4,8-9,11-27 updated.~{input_id}.gene-metrics.csv > ~{input_id}.gene-metrics.csv\n",
      "265c267,268\n",
      "< }\n",
      "\\ No newline at end of file\n",
      "---\n",
      "> }\n",
      "> \n"
     ]
    }
   ],
   "source": [
    "!diff ../scAtlas/tasks/Metrics_v6.0.0.wdl ../scAtlas/tasks/Metrics.wdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222d221\n",
      "<     String star_strand_mode\n",
      "228c227\n",
      "<     String docker = \"us.gcr.io/broad-gotc-prod/star:1.0.1-2.7.11a-1692706072\"\n",
      "---\n",
      ">     String docker = \"us.gcr.io/broad-gotc-prod/star:1.0.0-2.7.9a-1658781884\"\n",
      "245d243\n",
      "<     star_strand_mode: \"STAR mode for handling stranded reads. Options are 'Forward', 'Reverse, or 'Unstranded'\"\n",
      "281c279\n",
      "<         COUNTING_MODE=\"GeneFull_Ex50pAS\"\n",
      "---\n",
      ">         COUNTING_MODE=\"GeneFull\"\n",
      "286,294d283\n",
      "< # Check that the star strand mode matches STARsolo aligner options\n",
      "<     if [[ \"~{star_strand_mode}\" == \"Forward\" ]] || [[ \"~{star_strand_mode}\" == \"Reverse\" ]] || [[ \"~{star_strand_mode}\" == \"Unstranded\" ]]\n",
      "<     then\n",
      "<         ## single cell or whole cell\n",
      "<         echo STAR mode is assigned\n",
      "<     else\n",
      "<         echo Error: unknown STAR strand mode: \"~{star_strand_mode}\". Should be Forward, Reverse, or Unstranded.\n",
      "<         exit 1;\n",
      "<     fi\n",
      "307c296\n",
      "<       --soloStrand ~{star_strand_mode} \\\n",
      "---\n",
      ">       --soloStrand Unstranded \\\n",
      "320,322c309,310\n",
      "<       --outSAMattributes UB UR UY CR CB CY NH GX GN sF \\\n",
      "<       --soloBarcodeReadLength 0 \\\n",
      "<       --soloCellReadStats Standard\n",
      "---\n",
      ">       --outSAMattributes UB UR UY CR CB CY NH GX GN \\\n",
      ">       --soloBarcodeReadLength 0\n",
      "327c315\n",
      "<       --soloStrand ~{star_strand_mode} \\\n",
      "---\n",
      ">       --soloStrand Unstranded \\\n",
      "340,342c328,329\n",
      "<       --outSAMattributes UB UR UY CR CB CY NH GX GN sF \\\n",
      "<       --soloBarcodeReadLength 0 \\\n",
      "<       --soloCellReadStats Standard\n",
      "---\n",
      ">       --outSAMattributes UB UR UY CR CB CY NH GX GN \\\n",
      ">       --soloBarcodeReadLength 0\n",
      "344d330\n",
      "< \n",
      "348,351d333\n",
      "<     touch CellReads_sn_rna.stats\n",
      "<     touch Features_sn_rna.stats\n",
      "<     touch Summary_sn_rna.csv\n",
      "<     touch UMIperCellSorted_sn_rna.txt\n",
      "358,361d339\n",
      "<       mv \"Solo.out/Gene/CellReads.stats\" CellReads.stats\n",
      "<       mv \"Solo.out/Gene/Features.stats\" Features.stats\n",
      "<       mv \"Solo.out/Gene/Summary.csv\" Summary.csv\n",
      "<       mv \"Solo.out/Gene/UMIperCellSorted.txt\" UMIperCellSorted.txt\n",
      "366,372c344,346\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/raw/barcodes.tsv\" barcodes.tsv\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/raw/features.tsv\" features.tsv\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/raw/matrix.mtx\"   matrix.mtx\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/CellReads.stats\" CellReads.stats\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/Features.stats\" Features.stats\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/Summary.csv\" Summary.csv\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/UMIperCellSorted.txt\" UMIperCellSorted.txt\n",
      "---\n",
      ">         mv \"Solo.out/GeneFull/raw/barcodes.tsv\" barcodes.tsv\n",
      ">         mv \"Solo.out/GeneFull/raw/features.tsv\" features.tsv\n",
      ">         mv \"Solo.out/GeneFull/raw/matrix.mtx\"   matrix.mtx\n",
      "374,380c348,350\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/raw/barcodes.tsv\" barcodes.tsv\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/raw/features.tsv\" features.tsv\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/raw/matrix.mtx\"   matrix.mtx\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/CellReads.stats\" CellReads.stats\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/Features.stats\" Features.stats\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/Summary.csv\" Summary.csv\n",
      "<         mv \"Solo.out/GeneFull_Ex50pAS/UMIperCellSorted.txt\" UMIperCellSorted.txt\n",
      "---\n",
      ">         mv \"Solo.out/GeneFull/raw/barcodes.tsv\" barcodes.tsv\n",
      ">         mv \"Solo.out/GeneFull/raw/features.tsv\" features.tsv\n",
      ">         mv \"Solo.out/GeneFull/raw/matrix.mtx\"   matrix.mtx\n",
      "384,387d353\n",
      "<         mv \"Solo.out/Gene/CellReads.stats\" CellReads_sn_rna.stats\n",
      "<         mv \"Solo.out/Gene/Features.stats\" Features_sn_rna.stats\n",
      "<         mv \"Solo.out/Gene/Summary.csv\" Summary_sn_rna.csv\n",
      "<         mv \"Solo.out/Gene/UMIperCellSorted.txt\" UMIperCellSorted_sn_rna.txt\n",
      "415,422d380\n",
      "<     File cell_reads = \"CellReads.stats\"\n",
      "<     File align_features = \"Features.stats\"\n",
      "<     File summary = \"Summary.csv\"\n",
      "<     File umipercell = \"UMIperCellSorted.txt\"\n",
      "<     File cell_reads_sn_rna = \"CellReads_sn_rna.stats\"\n",
      "<     File align_features_sn_rna = \"Features_sn_rna.stats\"\n",
      "<     File summary_sn_rna = \"Summary_sn_rna.csv\"\n",
      "<     File umipercell_sn_rna = \"UMIperCellSorted_sn_rna.txt\"\n",
      "432,436d389\n",
      "<     Array[File]? cell_reads\n",
      "<     Array[File]? summary\n",
      "<     Array[File]? align_features\n",
      "<     Array[File]? umipercell\n",
      "<     \n",
      "463,490d415\n",
      "<     declare -a cell_reads_files=(~{sep=' ' cell_reads})\n",
      "<     declare -a summary_files=(~{sep=' ' summary})\n",
      "<     declare -a align_features_files=(~{sep=' ' align_features})\n",
      "<     declare -a umipercell_files=(~{sep=' ' umipercell})\n",
      "< \n",
      "<     for cell_read in \"${cell_reads_files[@]}\"; do\n",
      "<       if [ -f \"$cell_read\" ]; then\n",
      "<         cat \"$cell_read\" >> \"~{input_id}_cell_reads.txt\"\n",
      "<       fi\n",
      "<     done\n",
      "<     \n",
      "<     for summary in \"${summary_files[@]}\"; do\n",
      "<       if [ -f \"$summary\" ]; then\n",
      "<         cat \"$summary\" >> \"~{input_id}_summary.txt\"\n",
      "<       fi\n",
      "<     done\n",
      "<     \n",
      "<     for align_feature in \"${align_features_files[@]}\"; do\n",
      "<       if [ -f \"$align_feature\" ]; then\n",
      "<         cat \"$align_feature\" >> \"~{input_id}_align_features.txt\"\n",
      "<       fi\n",
      "<     done\n",
      "<  \n",
      "<     for umipercell in \"${umipercell_files[@]}\"; do\n",
      "<       if [ -f \"$umipercell\" ]; then\n",
      "<         cat \"$umipercell\" >> \"~{input_id}_umipercell.txt\"\n",
      "<       fi\n",
      "<     done\n",
      "492,504d416\n",
      "<     for umipercell in \"${umipercell_files[@]}\"; do\n",
      "<       if [ -f \"$umipercell\" ]; then\n",
      "<         cat \"$umipercell\" >> \"~{input_id}_umipercell.txt\"\n",
      "<       fi\n",
      "<     done\n",
      "<     \n",
      "<     # If text files are present, create a tar archive with them\n",
      "<     if ls *.txt 1> /dev/null 2>&1; then\n",
      "<       tar -zcvf ~{input_id}.star_metrics.tar *.txt\n",
      "<     else\n",
      "<       echo \"No text files found in the folder.\"\n",
      "<     fi\n",
      "< \n",
      "526d437\n",
      "<     File? cell_reads_out = \"~{input_id}.star_metrics.tar\"\n",
      "541c452\n",
      "<     String docker = \"us.gcr.io/broad-gotc-prod/star:1.0.1-2.7.11a-1692706072\"\n",
      "---\n",
      ">     String docker = \"us.gcr.io/broad-gotc-prod/star:1.0.0-2.7.9a-1658781884\"\n",
      "605c516\n",
      "<       --outSAMattributes UB UR UY CR CB CY NH GX GN sF\n",
      "---\n",
      ">       --outSAMattributes UB UR UY CR CB CY NH GX GN\n"
     ]
    }
   ],
   "source": [
    "!diff ../scAtlas/tasks/StarAlign_v6.0.0.wdl ../scAtlas/tasks/StarAlign.wdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!diff ../scAtlas/tasks/MergeSortBam_v6.0.0.wdl ../scAtlas/tasks/MergeSortBam.wdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff: ../scAtlas/tasks/RunEmptyDrops_v6.0.0.wdl: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!diff ../scAtlas/tasks/RunEmptyDrops_v6.0.0.wdl ../scAtlas/tasks/RunEmptyDrops.wdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Pipeline validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "List of Workflow dependencies is:\n",
      "/Users/xiliu/Documents/analysis/terraPipelines/scAtlas/tasks/CheckInputs.wdl\n",
      "/Users/xiliu/Documents/analysis/terraPipelines/scAtlas/tasks/FastqProcessing.wdl\n",
      "/Users/xiliu/Documents/analysis/terraPipelines/scAtlas/tasks/RunEmptyDrops.wdl\n",
      "/Users/xiliu/Documents/analysis/terraPipelines/scAtlas/tasks/cellbender_remove_background.wdl\n",
      "/Users/xiliu/Documents/analysis/terraPipelines/scAtlas/tasks/StarAlign.wdl\n",
      "/Users/xiliu/Documents/analysis/terraPipelines/scAtlas/tasks/H5adUtils.wdl\n",
      "/Users/xiliu/Documents/analysis/terraPipelines/scAtlas/tasks/MergeSortBam.wdl\n",
      "/Users/xiliu/Documents/analysis/terraPipelines/scAtlas/tasks/Metrics.wdl\n"
     ]
    }
   ],
   "source": [
    "### Syntex validation ###\n",
    "!java -jar ../../womtool/womtool-85.jar validate -l ../scAtlas/optimus/Optimus.wdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-20 11:08:52,00] [info] Running with database db.url = jdbc:hsqldb:mem:a11f2969-e780-4c5f-bcd0-4ef809066ba0;shutdown=false;hsqldb.tx=mvcc\n",
      "[2023-09-20 11:08:53,18] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000\n",
      "[2023-09-20 11:08:53,20] [info] [RenameWorkflowOptionsInMetadata] 100%\n",
      "[2023-09-20 11:08:53,30] [info] Running with database db.url = jdbc:hsqldb:mem:95187750-31e2-4b5c-8805-e8786fac8cd7;shutdown=false;hsqldb.tx=mvcc\n",
      "[2023-09-20 11:08:53,48] [info] Slf4jLogger started\n",
      "[2023-09-20 11:08:53,56] [info] Workflow heartbeat configuration:\n",
      "{\n",
      "  \"cromwellId\" : \"cromid-cfcc906\",\n",
      "  \"heartbeatInterval\" : \"2 minutes\",\n",
      "  \"ttl\" : \"10 minutes\",\n",
      "  \"failureShutdownDuration\" : \"5 minutes\",\n",
      "  \"writeBatchSize\" : 10000,\n",
      "  \"writeThreshold\" : 10000\n",
      "}\n",
      "[2023-09-20 11:08:53,58] [info] Metadata summary refreshing every 1 second.\n",
      "[2023-09-20 11:08:53,58] [info] No metadata archiver defined in config\n",
      "[2023-09-20 11:08:53,58] [info] No metadata deleter defined in config\n",
      "[2023-09-20 11:08:53,59] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.\n",
      "[2023-09-20 11:08:53,59] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.\n",
      "[2023-09-20 11:08:53,60] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.\n",
      "[2023-09-20 11:08:53,63] [info] JobRestartCheckTokenDispenser - Distribution rate: 50 per 1 seconds.\n",
      "[2023-09-20 11:08:53,63] [info] JobExecutionTokenDispenser - Distribution rate: 20 per 10 seconds.\n",
      "[2023-09-20 11:08:53,67] [info] SingleWorkflowRunnerActor: Version 85\n",
      "[2023-09-20 11:08:53,67] [info] SingleWorkflowRunnerActor: Submitting workflow\n",
      "[2023-09-20 11:08:53,69] [info] Unspecified type (Unspecified version) workflow bb42d86f-4211-4639-a37f-a1e61feb3d53 submitted\n",
      "[2023-09-20 11:08:53,70] [info] SingleWorkflowRunnerActor: Workflow submitted \u001b[38;5;2mbb42d86f-4211-4639-a37f-a1e61feb3d53\u001b[0m\n",
      "[2023-09-20 11:08:53,71] [info] 1 new workflows fetched by cromid-cfcc906: bb42d86f-4211-4639-a37f-a1e61feb3d53\n",
      "[2023-09-20 11:08:53,71] [info] WorkflowManagerActor: Starting workflow \u001b[38;5;2mbb42d86f-4211-4639-a37f-a1e61feb3d53\u001b[0m\n",
      "[2023-09-20 11:08:53,71] [info] WorkflowManagerActor: Successfully started WorkflowActor-bb42d86f-4211-4639-a37f-a1e61feb3d53\n",
      "[2023-09-20 11:08:53,71] [info] Retrieved 1 workflows from the WorkflowStoreActor\n",
      "[2023-09-20 11:08:53,73] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.\n",
      "[2023-09-20 11:08:53,76] [info] MaterializeWorkflowDescriptorActor [\u001b[38;5;2mbb42d86f\u001b[0m]: Parsing workflow as WDL 1.0\n",
      "[2023-09-20 11:08:54,29] [info] WorkflowManagerActor: Workflow bb42d86f-4211-4639-a37f-a1e61feb3d53 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:\n",
      "Required workflow input 'Optimus.input_id' not specified\n",
      "Required workflow input 'Optimus.tenx_chemistry_version' not specified\n",
      "Required workflow input 'Optimus.r1_fastq' not specified\n",
      "Required workflow input 'Optimus.annotations_gtf' not specified\n",
      "Required workflow input 'Optimus.tar_star_reference' not specified\n",
      "Required workflow input 'Optimus.ref_genome_fasta' not specified\n",
      "Required workflow input 'Optimus.r2_fastq' not specified\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:257)\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:227)\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:222)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat akka.actor.FSM.processEvent(FSM.scala:707)\n",
      "\tat akka.actor.FSM.processEvent$(FSM.scala:704)\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:169)\n",
      "\tat akka.actor.LoggingFSM.processEvent(FSM.scala:847)\n",
      "\tat akka.actor.LoggingFSM.processEvent$(FSM.scala:829)\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:169)\n",
      "\tat akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701)\n",
      "\tat akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695)\n",
      "\tat akka.actor.Actor.aroundReceive(Actor.scala:539)\n",
      "\tat akka.actor.Actor.aroundReceive$(Actor.scala:537)\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:169)\n",
      "\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:614)\n",
      "\tat akka.actor.ActorCell.invoke(ActorCell.scala:583)\n",
      "\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268)\n",
      "\tat akka.dispatch.Mailbox.run(Mailbox.scala:229)\n",
      "\tat akka.dispatch.Mailbox.exec(Mailbox.scala:241)\n",
      "\tat akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n",
      "\tat akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n",
      "\tat akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n",
      "\tat akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
      "\n",
      "[2023-09-20 11:08:58,64] [info] Not triggering log of restart checking token queue status. Effective log interval = None\n",
      "[2023-09-20 11:08:58,64] [info] Not triggering log of execution token queue status. Effective log interval = None\n",
      "[2023-09-20 11:08:58,66] [info] WorkflowManagerActor: Workflow actor for bb42d86f-4211-4639-a37f-a1e61feb3d53 completed with status 'Failed'. The workflow will be removed from the workflow store.\n",
      "[2023-09-20 11:09:01,83] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.\n",
      "[2023-09-20 11:09:03,61] [info] Workflow polling stopped\n",
      "[2023-09-20 11:09:03,62] [info] 0 workflows released by cromid-cfcc906\n",
      "[2023-09-20 11:09:03,62] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds\n",
      "[2023-09-20 11:09:03,62] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds\n",
      "[2023-09-20 11:09:03,63] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds\n",
      "[2023-09-20 11:09:03,63] [info] Aborting all running workflows.\n",
      "[2023-09-20 11:09:03,63] [info] JobExecutionTokenDispenser stopped\n",
      "[2023-09-20 11:09:03,63] [info] WorkflowStoreActor stopped\n",
      "[2023-09-20 11:09:03,63] [info] WorkflowLogCopyRouter stopped\n",
      "[2023-09-20 11:09:03,63] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds\n",
      "[2023-09-20 11:09:03,63] [info] WorkflowManagerActor: All workflows finished\n",
      "[2023-09-20 11:09:03,63] [info] WorkflowManagerActor stopped\n",
      "[2023-09-20 11:09:03,74] [info] Connection pools shut down\n",
      "[2023-09-20 11:09:03,74] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:03,74] [info] Shutting down JobStoreActor - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:03,74] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:03,74] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:03,74] [info] Shutting down DockerHashActor - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:03,74] [info] Shutting down IoProxy - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:03,74] [info] SubWorkflowStoreActor stopped\n",
      "[2023-09-20 11:09:03,74] [info] JobStoreActor stopped\n",
      "[2023-09-20 11:09:03,74] [info] CallCacheWriteActor Shutting down: 0 queued messages to process\n",
      "[2023-09-20 11:09:03,74] [info] KvWriteActor Shutting down: 0 queued messages to process\n",
      "[2023-09-20 11:09:03,74] [info] WriteMetadataActor Shutting down: 0 queued messages to process\n",
      "[2023-09-20 11:09:03,74] [info] CallCacheWriteActor stopped\n",
      "[2023-09-20 11:09:03,75] [info] IoProxy stopped\n",
      "[2023-09-20 11:09:03,75] [info] ServiceRegistryActor stopped\n",
      "[2023-09-20 11:09:03,75] [info] DockerHashActor stopped\n",
      "[2023-09-20 11:09:03,76] [info] Database closed\n",
      "[2023-09-20 11:09:03,76] [info] Stream materializer shut down\n",
      "[2023-09-20 11:09:03,77] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false\n",
      "[2023-09-20 11:09:03,77] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false\n",
      "[2023-09-20 11:09:03,77] [info] WDL HTTP import resolver closed\n",
      "[2023-09-20 11:09:03,77] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false\n",
      "Workflow bb42d86f-4211-4639-a37f-a1e61feb3d53 transitioned to state Failed\n"
     ]
    }
   ],
   "source": [
    "### Running the workflow\n",
    "!java -jar ../../cromwell/cromwell-85.jar run ../scAtlas/optimus/Optimus.wdl -i ../scAtlas/optimus/test_data/10k_pbmc_v3_outputs.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-20 11:09:04,54] [info] Running with database db.url = jdbc:hsqldb:mem:d9a59d38-cd15-4de2-a6fb-b9a832f6839b;shutdown=false;hsqldb.tx=mvcc\n",
      "[2023-09-20 11:09:05,72] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000\n",
      "[2023-09-20 11:09:05,73] [info] [RenameWorkflowOptionsInMetadata] 100%\n",
      "[2023-09-20 11:09:05,85] [info] Running with database db.url = jdbc:hsqldb:mem:cbf3ff03-2e69-4803-b218-4eb50c03133e;shutdown=false;hsqldb.tx=mvcc\n",
      "[2023-09-20 11:09:06,04] [info] Slf4jLogger started\n",
      "[2023-09-20 11:09:06,12] [info] Workflow heartbeat configuration:\n",
      "{\n",
      "  \"cromwellId\" : \"cromid-b1aec6f\",\n",
      "  \"heartbeatInterval\" : \"2 minutes\",\n",
      "  \"ttl\" : \"10 minutes\",\n",
      "  \"failureShutdownDuration\" : \"5 minutes\",\n",
      "  \"writeBatchSize\" : 10000,\n",
      "  \"writeThreshold\" : 10000\n",
      "}\n",
      "[2023-09-20 11:09:06,14] [info] Metadata summary refreshing every 1 second.\n",
      "[2023-09-20 11:09:06,15] [info] No metadata archiver defined in config\n",
      "[2023-09-20 11:09:06,15] [info] No metadata deleter defined in config\n",
      "[2023-09-20 11:09:06,15] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.\n",
      "[2023-09-20 11:09:06,15] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.\n",
      "[2023-09-20 11:09:06,16] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.\n",
      "[2023-09-20 11:09:06,19] [info] JobRestartCheckTokenDispenser - Distribution rate: 50 per 1 seconds.\n",
      "[2023-09-20 11:09:06,20] [info] JobExecutionTokenDispenser - Distribution rate: 20 per 10 seconds.\n",
      "[2023-09-20 11:09:06,24] [info] SingleWorkflowRunnerActor: Version 85\n",
      "[2023-09-20 11:09:06,24] [info] SingleWorkflowRunnerActor: Submitting workflow\n",
      "[2023-09-20 11:09:06,27] [info] Unspecified type (Unspecified version) workflow f53820eb-d61a-4bcf-81ae-0dc6d0044840 submitted\n",
      "[2023-09-20 11:09:06,28] [info] SingleWorkflowRunnerActor: Workflow submitted \u001b[38;5;2mf53820eb-d61a-4bcf-81ae-0dc6d0044840\u001b[0m\n",
      "[2023-09-20 11:09:06,28] [info] 1 new workflows fetched by cromid-b1aec6f: f53820eb-d61a-4bcf-81ae-0dc6d0044840\n",
      "[2023-09-20 11:09:06,28] [info] WorkflowManagerActor: Starting workflow \u001b[38;5;2mf53820eb-d61a-4bcf-81ae-0dc6d0044840\u001b[0m\n",
      "[2023-09-20 11:09:06,29] [info] WorkflowManagerActor: Successfully started WorkflowActor-f53820eb-d61a-4bcf-81ae-0dc6d0044840\n",
      "[2023-09-20 11:09:06,29] [info] Retrieved 1 workflows from the WorkflowStoreActor\n",
      "[2023-09-20 11:09:06,30] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.\n",
      "[2023-09-20 11:09:06,33] [info] MaterializeWorkflowDescriptorActor [\u001b[38;5;2mf53820eb\u001b[0m]: Parsing workflow as WDL 1.0\n",
      "[2023-09-20 11:09:06,89] [info] WorkflowManagerActor: Workflow f53820eb-d61a-4bcf-81ae-0dc6d0044840 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:\n",
      "Failed to evaluate input 'r2_fastq' (reason 1 of 1): No coercion defined from '\"${[\\\"gs://whitelabgx-references/resources/pbmc_10k_10X_v3/pbmc_10k_v3_S1_L001_R2_001.fastq.gz\\\",\\\"gs://whitelabgx-references/resources/pbmc_10k_10X_v3/pbmc_10k_v3_S1_L002_R2_001.fastq.gz\\\"]}\"' of type 'spray.json.JsString' to 'Array[File]'.\n",
      "Failed to evaluate input 'i1_fastq' (reason 1 of 1): No coercion defined from '\"${[\\\"gs://whitelabgx-references/resources/pbmc_10k_10X_v3/pbmc_10k_v3_S1_L001_I1_001.fastq.gz\\\",\\\"gs://whitelabgx-references/resources/pbmc_10k_10X_v3/pbmc_10k_v3_S1_L002_I1_001.fastq.gz\\\"]}\"' of type 'spray.json.JsString' to 'Array[File]?'.\n",
      "Failed to evaluate input 'r1_fastq' (reason 1 of 1): No coercion defined from '\"${[\\\"gs://whitelabgx-references/resources/pbmc_10k_10X_v3/pbmc_10k_v3_S1_L001_R1_001.fastq.gz\\\",\\\"gs://whitelabgx-references/resources/pbmc_10k_10X_v3/pbmc_10k_v3_S1_L002_R1_001.fastq.gz\\\"]}\"' of type 'spray.json.JsString' to 'Array[File]'.\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:257)\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:227)\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:222)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat akka.actor.FSM.processEvent(FSM.scala:707)\n",
      "\tat akka.actor.FSM.processEvent$(FSM.scala:704)\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:169)\n",
      "\tat akka.actor.LoggingFSM.processEvent(FSM.scala:847)\n",
      "\tat akka.actor.LoggingFSM.processEvent$(FSM.scala:829)\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:169)\n",
      "\tat akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:701)\n",
      "\tat akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:695)\n",
      "\tat akka.actor.Actor.aroundReceive(Actor.scala:539)\n",
      "\tat akka.actor.Actor.aroundReceive$(Actor.scala:537)\n",
      "\tat cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:169)\n",
      "\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:614)\n",
      "\tat akka.actor.ActorCell.invoke(ActorCell.scala:583)\n",
      "\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268)\n",
      "\tat akka.dispatch.Mailbox.run(Mailbox.scala:229)\n",
      "\tat akka.dispatch.Mailbox.exec(Mailbox.scala:241)\n",
      "\tat akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n",
      "\tat akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n",
      "\tat akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n",
      "\tat akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n",
      "\n",
      "[2023-09-20 11:09:11,20] [info] Not triggering log of restart checking token queue status. Effective log interval = None\n",
      "[2023-09-20 11:09:11,22] [info] Not triggering log of execution token queue status. Effective log interval = None\n",
      "[2023-09-20 11:09:11,22] [info] WorkflowManagerActor: Workflow actor for f53820eb-d61a-4bcf-81ae-0dc6d0044840 completed with status 'Failed'. The workflow will be removed from the workflow store.\n",
      "[2023-09-20 11:09:12,36] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.\n",
      "[2023-09-20 11:09:16,17] [info] Workflow polling stopped\n",
      "[2023-09-20 11:09:16,19] [info] 0 workflows released by cromid-b1aec6f\n",
      "[2023-09-20 11:09:16,19] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds\n",
      "[2023-09-20 11:09:16,19] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds\n",
      "[2023-09-20 11:09:16,19] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds\n",
      "[2023-09-20 11:09:16,19] [info] Aborting all running workflows.\n",
      "[2023-09-20 11:09:16,19] [info] JobExecutionTokenDispenser stopped\n",
      "[2023-09-20 11:09:16,19] [info] WorkflowStoreActor stopped\n",
      "[2023-09-20 11:09:16,20] [info] WorkflowLogCopyRouter stopped\n",
      "[2023-09-20 11:09:16,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds\n",
      "[2023-09-20 11:09:16,20] [info] WorkflowManagerActor: All workflows finished\n",
      "[2023-09-20 11:09:16,20] [info] WorkflowManagerActor stopped\n",
      "[2023-09-20 11:09:16,29] [info] Connection pools shut down\n",
      "[2023-09-20 11:09:16,29] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:16,29] [info] Shutting down JobStoreActor - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:16,29] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:16,29] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:16,29] [info] Shutting down DockerHashActor - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:16,29] [info] SubWorkflowStoreActor stopped\n",
      "[2023-09-20 11:09:16,29] [info] Shutting down IoProxy - Timeout = 1800 seconds\n",
      "[2023-09-20 11:09:16,29] [info] JobStoreActor stopped\n",
      "[2023-09-20 11:09:16,29] [info] CallCacheWriteActor Shutting down: 0 queued messages to process\n",
      "[2023-09-20 11:09:16,29] [info] CallCacheWriteActor stopped\n",
      "[2023-09-20 11:09:16,29] [info] WriteMetadataActor Shutting down: 0 queued messages to process\n",
      "[2023-09-20 11:09:16,29] [info] KvWriteActor Shutting down: 0 queued messages to process\n",
      "[2023-09-20 11:09:16,30] [info] IoProxy stopped\n",
      "[2023-09-20 11:09:16,30] [info] ServiceRegistryActor stopped\n",
      "[2023-09-20 11:09:16,30] [info] DockerHashActor stopped\n",
      "[2023-09-20 11:09:16,30] [info] Database closed\n",
      "[2023-09-20 11:09:16,30] [info] Stream materializer shut down\n",
      "[2023-09-20 11:09:16,30] [info] WDL HTTP import resolver closed\n",
      "[2023-09-20 11:09:16,31] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false\n",
      "[2023-09-20 11:09:16,31] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false\n",
      "[2023-09-20 11:09:16,31] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false\n",
      "Workflow f53820eb-d61a-4bcf-81ae-0dc6d0044840 transitioned to state Failed\n"
     ]
    }
   ],
   "source": [
    "!java -jar ../../cromwell/cromwell-85.jar run ../scAtlas/optimus/Optimus.wdl -i ../scAtlas/optimus/test_data/inputs.json\n",
    "#inputs.json is downloaded directly form terra.bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Data Loading into terra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basicAtlas import terra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables definition\n",
    "GCP_BUCKET=\"gs://whitelabgx-references\"\n",
    "GCP_FOLDER_NAME=\"resources/pbmc_10k_10X_v3\"\n",
    "TERRA_WS = 'whitelabgx/scRNAseq'\n",
    "PROJECT=\"optimus_V6.0.0_wlg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please be sure you gave access to your terra email account access to this bucket\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r1_fastq</th>\n",
       "      <th>r2_fastq</th>\n",
       "      <th>i1_fastq</th>\n",
       "      <th>Source</th>\n",
       "      <th>participant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pbmc_10k_v3_S1_L001</th>\n",
       "      <td>gs://whitelabgx-references/resources/pbmc_10k_...</td>\n",
       "      <td>gs://whitelabgx-references/resources/pbmc_10k_...</td>\n",
       "      <td>gs://whitelabgx-references/resources/pbmc_10k_...</td>\n",
       "      <td>U</td>\n",
       "      <td>pbmc_10k_v3_S1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pbmc_10k_v3_S1_L002</th>\n",
       "      <td>gs://whitelabgx-references/resources/pbmc_10k_...</td>\n",
       "      <td>gs://whitelabgx-references/resources/pbmc_10k_...</td>\n",
       "      <td>gs://whitelabgx-references/resources/pbmc_10k_...</td>\n",
       "      <td>U</td>\n",
       "      <td>pbmc_10k_v3_S1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              r1_fastq  \\\n",
       "sample_id                                                                \n",
       "pbmc_10k_v3_S1_L001  gs://whitelabgx-references/resources/pbmc_10k_...   \n",
       "pbmc_10k_v3_S1_L002  gs://whitelabgx-references/resources/pbmc_10k_...   \n",
       "\n",
       "                                                              r2_fastq  \\\n",
       "sample_id                                                                \n",
       "pbmc_10k_v3_S1_L001  gs://whitelabgx-references/resources/pbmc_10k_...   \n",
       "pbmc_10k_v3_S1_L002  gs://whitelabgx-references/resources/pbmc_10k_...   \n",
       "\n",
       "                                                              i1_fastq Source  \\\n",
       "sample_id                                                                       \n",
       "pbmc_10k_v3_S1_L001  gs://whitelabgx-references/resources/pbmc_10k_...      U   \n",
       "pbmc_10k_v3_S1_L002  gs://whitelabgx-references/resources/pbmc_10k_...      U   \n",
       "\n",
       "                        participant  \n",
       "sample_id                            \n",
       "pbmc_10k_v3_S1_L001  pbmc_10k_v3_S1  \n",
       "pbmc_10k_v3_S1_L002  pbmc_10k_v3_S1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uploaded samples in google bucket associated with terra workspace\n",
    "terra.uploadFromFolder_10x(GCP_BUCKET + '/' + GCP_FOLDER_NAME + '/',\n",
    "                           TERRA_WS,\n",
    "                           samplesetname=PROJECT,\n",
    "                           fformat=\"fastqR1R2\",\n",
    "                           sep='_00|_00',\n",
    "                           loc =0,\n",
    "                           test=True) # test=True to test the function after upload successfully of the files to terra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bulk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
